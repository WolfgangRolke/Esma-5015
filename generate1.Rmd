---
header-includes: \usepackage{color}
                 \usepackage{float}
output:
  pdf_document:
    fig_caption: no
  html_document: default
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
source("../R/setup.rmd.R", local=TRUE)
setup.rmd(local.env=environment())
```
`r hl()$basefontsize()`
`r hl()$style()`

# Generating Random Variables

## General Methods 

### Random Numbers

Everything starts with generating X~1~, X~2~, .. iid U[0,1]. There are simply called random numbers. There are some ways to get these:

- random number tables

- numbers taken from things like the exact (computer) time

- quantum random number generators

- ...

The R package random has the routine randomNumbers which gets random numbers from a web site which generates them based on (truely random) admospheric phenomena.

```{r}
require(random)
randomNumbers(20, 0, 100)
```


### Pseudo-Random Numbers

These are numbers that look random, smell random ...

Of course a computer can not do anything truly random, so all we can do is generate X~1~, X~2~, .. that **appear** to be iid U[0,1], so-called pseudo-random numbers.

Luckily, some people are really good at that!

####**Example** 
A *linear congruential generator* works as follows:
 
start with a **seed** X~0~, calculate 

X~n+1~=(aX~n~+c) mod m

where a,  c and m are chosen such that

- c and m are relatively prime

- a-1 is divisible by all prome factors of m

- a-1 is a multiple of 4 if m is a multiple of 4

A well known algorithm called PRNG in Numerical Recipes in C uses a=1664525,  c=1013904223 and m=2^32^

Some computer programs (like R) have them already built in, most general computer languages (like C) do not. There are many excellent ones availabe on the Internet. 

Some issues to be aware of:

- All pseudo random number generators are cyclic, that is there is an N such that X~1~=X~N~,  X~2~=X~N+1~ etc. For any decent method we have N in the billions. For the one above N=m=2^32^

- All pseudo random number generators have a SEED, usually an integer. If you want to generate the same sequence you can do this by specifying this SEED.

in R if you want to generate the same sequence again then use the command set.seed(SEED) where SEED is an integer.

```{r}
sample(1:3, size=10, replace=TRUE)
sample(1:3, size=10, replace=TRUE)
set.seed(1111)
sample(1:3, size=10, replace=TRUE)
set.seed(1111)
sample(1:3, size=10, replace=TRUE)
```

This can be very useful when writing a simulation and getting an error every 10000 or so runs!

- There are often subtle differences between compilers so don't expect the same program to generate the same sequence on different computers.

### Generating Discrete Random Variables


-  **The Inverse Transform Method**

Say we want to generate a random variable X from a distribution with density
$$
f(x_j)=P(X=x_j)=p_j 
$$
j = 1, 2, 3, ..


Here is a simple algorithm to do this:

Step 1: generate $U\sim U[0,1]$, set j = 1, p = p~1~  
Step 2: if U < p, set X = x~j~, **done**  
Step 3: set j = j+1, p = p+p~j~, goto Step 2  

Why this works:

![](graphs/gen11.png)

Here we have p~1~ < U < p~1~ + p~2~, so we set X = x~2~

The routine **gendisc1** runs this algorithm:

```{r}
gendisc1 <- function (n, x, p) {
  y <- rep(0, n)
  m <- length(x)
  p <- p/sum(p)
  cdf <- cumsum(p)
  for(i in 1:n) {
    U <- runif(1)
    for(j in 1:m) {
        if(U < cdf[j]) {
            y[i] <- x[j]
            break
        }
    }
  }
  y
}
table(gendisc1(n=1000, x=letters[1:5], p=1:5))/1000
```

How this works: 

generate data from the following rv: 

P(X = 0) = 0.1,  P(X = 1) = 0.3, P(X = 2) = 0.5 and P(X = 4) = 0.1

Now we generate U~U[0,1] and we get:

Case 1: U=0.0512, then we have the following:  
![](graphs/gen110.png)  
so U < p~1~, and we set X = x~1~ = 0

Case 2: U=0.3502, then p~1~ < U < p~1~+p~2~, and we set X = x~2~ = 1

Case 3: U = 0.9542, then U > p~1~+p~2~+..+p~k-1~, and we set X = x~4~ = 4

`r hl()$hr()`

Notice that the values of X (x~1~,..) are almost irrelevant, in fact we can just generate data with values 1, 2, .., and in the end change the "labels": "1"&rarr;x~1~, "2"&rarr;x~2~ etc.

**Theorem** the above algorithm generates the required rv.  
**proof**  
Remember that $U\sim U[0,1]$, so P(U < x) = x if 0 < x < 1

P(X = x~1~) = P(U < p~1~) = p~1~

say k>1, then 

P(X = x~k~) = P(p~1~+..+p~k-1~ < U < p~1~+..+p~k~) =  
(p~1~+..+p~k~) - (p~1~+..+p~k-1~) = p~k~

####**Example** 
Generate 100000 observations from $X\sim Bin(10, 0.65)$ 

Of course there is a routine in R built in to do this: 
```{r, eval=FALSE}
rbinom(10000, 10, 0.65)
```

You can check that the correct data was generated by comparing
```{r}
tmp <- proc.time()
x <- table(rbinom(100000, 10, 0.65))
proc.time() - tmp
y <- round(100000*dbinom(0:10, 10, 0.65))
rbind(x, y)
```

Or you can use our routine above:
```{r}
tmp <- proc.time() 
x <- table(gendisc1(100000, 0:10, dbinom(0:10, 10, 0.65)))
proc.time() - tmp
rbind(x, y)
```

So this works but is a bit slow. Can we speed it up? Consider this:

$\text{dbinom}(0,10,0.65) = 2.76 \times 10^{-5}$, so we almost never choose 0, but we check it in the computer program every single time.

dbinom(6,10,0.65) - dbinom(5,10,0.65) = 0.2522 is the biggest interval and about 25\% of the time U is in there, but in order to get there our program first needs to check 0, 1, 2, ,3 , 4 and 5. 

This give us an idea for a slight improvement:  

order p and x by decreasing size of p. Here is the table of x and p:

```{r echo=FALSE}
x <- 0:10
p <- round(dbinom(x, 10, 0.65), 3)
df <- data.frame(x=x, p=p)
kable.nice(df)
```


Here is the same table, ordered by the p's:

```{r echo=FALSE}
df <- df[order(df$p, decreasing = TRUE), ]
rownames(df) <- NULL
kable.nice(df)
```

so now if U < 0.252 we set x = 7, if 0.252 < U < 0.252+0.238 = 0.49 set x = 6 and so on

The routine **gendisc2** does this. 

```{r}
gendisc2 <- function (n, x, p) {
  y <- rep(0, n)
  m <- length(x)
  x <- x[order(p, decreasing = TRUE)]
  p <- sort(p, decreasing = TRUE)
  print(rbind(x, p))
  p <- cumsum(p)
  for (i in 1:n) {
      U <- runif(1)
      for (j in 1:m) {
          if (U < p[j]) {
              y[i] <- x[j]
              break
          }
      }
  }
  y
}
```


Check
```{r}
tmp <- proc.time() 
x <- table(gendisc2(100000, 0:10, 
                    dbinom(0:10, 10, 0.65)))
proc.time() - tmp
rbind(x, y)
```

**rbinom** is still much faster. Another way in R to speed things up is to "vectorize" the program.

####**Example** 
Generate observations from $X\sim G(0.01)$. 

Here we have the additional problem that the vector p is infinite.

Computers cannot handle infinitly large objects, so we need to "truncate" p. Here is one way to do this: 

1. Find x~1~ and x~2~ such that P(x~1~ < X < x~2~) = 0.999999  

```{r}
qgeom(c(0.0000005, 0.9999995), 0.01) + 1
```

If U < 0.0000005 + P(X = 1) = 0.0100005, set x = 1

If U > 0.9999995, set x = 1444

otherwise do as above.

- **The Accept-Reject Algorithm**

Suppose we have a method for generating a random variable having density \{q~j~, j = 1, 2, ..\} and we want to generate r.v. from a distribution with density \{p~j~, j = 1, 2, ..\}. We can do this by first simulating a r.v. Y from \{q~j~\} and then "accepting" this simulated value with probability proportional to p~Y~/q~Y~.
  
  
Specifically, let c be a constant such that  $p_j/q_j \le c$ for all j such that p~j~ > 0. Then 

Step 1: generate Y from density \{q~j~\} 

Step 2: generate $U\sim U[0,1]$ 

Step 3: If U < p~Y~/(cq~Y~), set X = Y and stop. Otherwise go back to 1 

**Notation** Y is often called an *auxiliary* variable. In a somewhat different context later it will also be called a *proposal density*, and sometimes either of these terms is used. 

####**Example** 
Say we want to generate a r.v X with values x in \{1, 3, 5, 7\} and probabilities  p = (0.1, 0.5, 0.1, 0.3).  

First we need a r.v. Y which is easy to generate and takes 4 values (not necessarily the same as in X though!). We can use for this the r.v. that chooses a number from 1 to 4 at random, using the sample(1:4, 1) command. This has density q = (1/4, 1/4, 1/4, 1/4), so 

p/q = (0.4, 2, 0.4, 1.2) 

and if we set c = 2 we have 

p~j~/q~j~ $\le$ c for all j.
  
with this the accept-reject algorithm for this problem is implemented in **gendisc3**:

```{r}
gendisc3 <- function (n, x, p) {
  z <- rep(0, n)
  m <- length(x)
  q <- rep(1/4, 4)
  for (i in 1:n) {
      for (j in 1:100) {
          U <- runif(1)
          Y <- sample(1:4, 1)
          if (U < (p[Y]/(2*q[Y]))) {
              z[i] <- x[Y]
              break
          }
      }
  }
  z
}
table(gendisc3(n=10000, 
      x=c(1, 3, 5, 7), 
      p=c(0.1, 0.5, 0.1, 0.3)))/10000
```


Why this works: if our "candidate" r.v Y picks a value y which has a high probability in p, it will often be accepted and we get many of these. If on the other hand Y picks a value which has a low probability in p, it will rarely be accepted and we get only a few of those. The method is illustrated in **accrej.ill**.

```{r}
accrej.ill <- function (n) {
  x <- c(1, 3, 5, 7)
  p <- c(0.1, 0.5, 0.1, 0.3)
  q <- rep(1/4, 4)
  X <- rep(0, n)
  plot(c(0, 8), c(0, 1), type = "n", 
       xlab = "x", ylab = "P/(cq)")
  segments(x - 0.2, 2 * p, x + 0.2, 2 * p, lwd = 3)
  for (i in 1:n) {
      for (j in 1:100) {
          Y <- sample(1:4, 1)
          U <- runif(1)
          if (U < p[Y]/(2 * q[Y])) {
              X[i] <- x[Y]
              points(x[Y], U, pch = "x")
              break
          }
          else {
              points(x[Y], U, pch = "o")
          }
      }
  }
  X
}
```


```{r}
accrej.ill(10)
```


**Theorem**

The accept-reject algorithm generates a r.v. X such that 

P(X = x~j~) = p~j~

In addition, the number of iterations of the algorithm needed until X is found is a geometric r.v. with mean c.

*proof*

$$
\begin{aligned}
&P(Y=j, Y \text{ is accepted})    = \\
&P(Y=j)P(Y \text{ is accepted}|Y=j)    = \\
&q_j\frac{p_j}{cq_j}    = \frac{p_j}{c}\\
&P(Y \text{ is accepted}) = \\
&\sum_{j=1}^\infty P(Y=j, Y \text{ is accepted})    = \\
&\sum_{j=1}^\infty \frac{p_j}{c} =\frac1{c}
\end{aligned}
$$

Now each iteration is a Bernoulli trial with success probability 1/c, and successive trials are independent. Therefore the number of trials needed until the first success is a geometric r.v. with mean c. Also

$$
\begin{aligned}
&P(X=x_j)    = \\
&\sum_n P(j \text{ is accpeted at }n^{th} \text{ trial})    = \\
&\sum_n (1-1/c)^{n-1}\frac{p_j}{c}    = p_j\\
\end{aligned}
$$

####**Example**
say we want to generate a rv X with P(X = k) = a~k~, k = 1, 2, .., N for some fixed and known N > 1. (Of course the sample command with the argument prob = .. will do just that, but let's write our own routine).

Note that we here we know the p~j~'s only up to a constant.

For the rv Y we will simply use U[1, 2, .., N], that is

P(Y = k) = 1/N

Now
$$
P(X=j)=a_{j}, j=1,..,N; a_{j}\geq 0 \\ 
p_{j}=\frac{a_{j}}{\sum a_{i}} \\ 
P(Y=j)=\frac{1}{N}, j=1,..,N \\ 
\frac{p_{j}}{q_{j}}=\frac{Na_{j}}{\sum a_{i}} \\ 
\max \{\frac{p_{j}}{q_{j}}\}=\max \{\frac{Na_{j}}{\sum \\ a_{i}}\}=\frac{N\max \{a_{j}\}}{\sum a_{i}} \\
\frac{p_{j}}{cq_{j}}=\left( \frac{Na_{j}}{\sum a_{i}}\right) /\left( \frac{ N\max \{a_{j}\}}{\sum a_{i}}\right) =\frac{a_{j}}{\max \{a_{j}\}} 
$$
so we get that we don't actually need to find $\sum a_i$! In general we don't need to have the probabilities "normalized", that is sum up to 1.

Now for the routine:

```{r}
N <- 6
a <- c(1, 3, 1, 6, 10, 4)
n <- 10000
x <- rep(0, n)
counter <- 0
for (i in 1:n) {
  repeat {
    counter <- counter+1
    y <- sample( 1:N, 1)
    if (runif(1) <= a[y]/max(a)) {
       x[i] <- y
       break
    }
  }  
}
out <- data.frame( sim=c(table(x)), calc=a/sum(a)*n)
rownames(out) <- 1:N
out
c(counter/n, N*max(a)/sum(a))
```
Also the theorem says that a new X is generated every c = 2.4 tries, and the routine shows this to be true.

####**Example** 
say we want to generate r.v.'s X such that 

$P(X = k) = 6/(\pi^2 k^2), k=1,2, ...$

Recall

$\sum_{k=1}^\infty \frac1{k^2} = \frac{\pi^2}6$

We need a r.v Y on {1,2,..} which we can generate. There are two discrete rv's we know which have infinitely many values, the geometric and the Poisson. But there is a problem with those two:
 
![](graphs/gen112.png)

We do know, though, a continuous r.v. that goes to 0 very slowly, namely the Cauchy. Actually, its density $f(x)=1/(\pi (1+x^2))$ already has the right "size" for our problem. We can do this, then by "discretizing" the Cauchy: Let $Z\sim \text{Cauchy}$ and define

Y = i(I~[-i,-i+1]~(Z)+I~[i-1,i]~(Z)) for i=1,2,.. 

So

if |Z|<1 &rarr; Y=1

if 1<|Z|<2 &rarr; Y=2

and so on. Now

$$
\begin{aligned}
&q_i    = P(Y=i) =\\
&P(-i<Z<-i+1 \text{ or } i-1<Z<i)    = \\
&2P(i-1<Z<i)    = \\
&2\int_{i-1}^i \frac1{\pi(1+z^2)}dz=\\
&\frac2\pi \left(\arctan(i)-\arctan(i-1) \right)
\end{aligned}
$$

Now we need max\{p~j~/q~j~\}. Doing this via calculus would be quite difficult because we would end up with a nonlinear equation which we would need to solve numerically. A different approach is to just plot j vs. p~j~/q~j~ and see what it looks like:

```{r}
gendisc4 <- function (n, findc = FALSE) {
  if (findc) {
      i <- 1:n
      p <- 6/pi^2/i^2
      q <- 2/pi * (atan(i) - atan(i - 1))
      plot(i, p/q)
      return(max(p/q))
  }
  x <- rep(0, n)
  const <- 3/pi/1.216
  for (i in 1:n) {
      for (j in 1:100) {
          z <- rcauchy(1)
          y <- floor(abs(z)) + 1
          u <- runif(1)
          if (u <= const/(y^2*(atan(y)-atan(y-1)))) {
              x[i] <- y
              break
          }
      }
  }
  table(x)/n
}
gendisc4(100, TRUE)
```

We see that p~j~/q~j~ appears to approach a limit a little below 1 as j goes to infinity, but it has a value of 1.216 at j=1, so we can reasonably guess that c = 1.216 should work for us. (Of course we can verify all that by taking derivatives and using de L'Hospital's rule).

With this we can generate these r.v.'s, again using **gendisc4**:

```{r}
tmp <- gendisc4(10000)
head(tmp)
tail(tmp)
```


Alternatively we could of course have used the inverse transform method, but notice that in this r.v. occasionally we see very large values, and so the inverse transform method would be quite slow.

In the theorem above it says that the mean number of trials until the first success, which is the number of "tries" until we find an acceptable candidate, has a geometric rv with mean c. Obviously the smaller c is, the faster we find a Y that is accepted, the faster we generate our observations. 

####**Example** 
say we want to generate data from a discrete rv f with $P( X = x ) = c\exp (- x^{3/2} )$, x = 1, 2, ... Let's assume we know how to generate data from a G(0.5). Then 

$$
\begin{aligned}
&\frac{\exp(-x^{3/2})}{(1/2)^{x}}   =\\ 
&\frac{\exp(-x^{3/2})}{\exp (-x\log (2))}=\\
&\exp \left( x\log (2)-x^{3/2} \right) \\
& \frac{d}{dx} \left\{ \exp \left( x\log (2)-x^{3/2} \right)  \right\}= \\
&\exp \left( x\log (2)-x^{3/2} \right) \left( \log(2) - \frac32 \sqrt{x}\right)<0
\end{aligned}
$$

so max\{ r~j~/q~j~ \} = r~1~/ q~1~ = exp(-1)/0.5 = 0.7357589

So the algorithm is

repeat {  
&nbsp;&nbsp;   
&nbsp;&nbsp;if ( runif(1) < 2^y^exp(-y^3/2^)/0.7357589 ) { x<- y ; break }  
}

### Continuous Distributions


**Accept-Reject Algorithm**

This is very similar (actually the same) as the method for discrete r.v. Assume want to generate a r.v. X with density f. We have a way to generate a r.v. Y with density g. Let c be a constant such that

f(x)/g(x) $\le$ c for all x. 

Then the accept-reject algorithm is as follows: 

Step 1: generate Y from pdf g  
Step 2: generate $U\sim U[0,1]$   
Step 3: If U < f(Y)/(cg(Y)), set X = Y and stop. Otherwise go back to 1 

We have the same theorem as for the discrete case:

**Theorem**

The accept-reject algorithm generates a r.v. X with density f. 

In addition, the number of iterations of the algorithm needed until X is found is a geometric r.v. with mean c.
  
*proof* same as above.

**Note**: we do have to be careful because of course g(x) can be 0.

This is ok as long as f(x) is 0 as well but not if f(x) > 0.

Basically we need Y to live on the same set as X. (We say X and Y have the same *support*)

####**Example**
generate a r.v. X with density f(x) = 6x(1-x) 0 < x < 1.

Here we can use $Y\sim U[0,1]$, with g(x) = 1.

Let's find max\{f(x)/g(x) | 0 < x < 1\}. Taking derivatives we find 

d/dx \{6x(1-x)\} = 6-12x = 0, x = 1/2. 

This is a maximum (?) so we have 

$$
c=\max \left\{ \frac{f(x)}{g(x)};0<x<1\right\} =f(\frac{1}{2})=\frac{3}{2} \\
\frac{f(x)}{cg(x)} = \frac{6x(1-x)}{3/2*1}/ = 4x(1-x)
$$  

So here is the routine:

```{r}
gencont1 <- function(n, findc = FALSE, Show = FALSE) {
  if (findc) {
      x <- seq(0, 1, length = 100)
      plot(x, 6 * x * (1 - x), type = "l")
      return(max(6 * x * (1 - x)))
  }
  X <- rep(0, n)
  for (i in 1:n) {
      for (j in 1:100) {
          Y <- runif(1)
          if (runif(1) <= 4 * Y * (1 - Y)) {
              X[i] <- Y
              break
          }
      }
  }
  if (Show) {
      hist(X, 50, freq = FALSE, main="")
      x <- seq(0, 1, length=250)
      lines(x, 6*x*(1-x),
            lwd=2, col="blue")
  }
  X
}
tmp <- gencont1(10000, Show=TRUE)
```

why the accept-reject algorithm works is illustrated in **accrej1.ill**

```{r}
accrej1.ill <- function(n) {
  f <- function(x) 6*x*(1-x)
  g <- function(x) 1
  c <- 3/2
  x <- seq(0, 1, length = 100)
  plot(x, f(x)/(c * g(x)), 
       xlab = "x", ylab = "f/(cg)", type = "l",
        xlim = c(0, 1), ylim = c(0, 1))
  for(i in 1:n) {
    Y <- runif(1)
    U <- runif(1)
    if (U <= f(Y)/(c * g(Y))) {
      points(Y, U, pch = "X", col = 3)
    }
    else {
      points(Y, U, pch = "0", col = 2)
    }
  }  
    
}
accrej1.ill(50)
```

####**Example** 
generate a r.v. $X\sim \text{Gamma}(3/2, 1)$.

Now f(x) > 0 for x > 0, so we need a Y that "lives" on $(0, \infty)$ and that we already know how to generate. Let's say we know how to generate $Y\sim Exp(1/\lambda)$.

What value of $\lambda$ should we use? Note that 

EX = 3/2

EY = $\lambda$

so maybe $\lambda=3/2$ is a good idea. 

with this we need to find c. Again we will try to find max\{f(x)/g(x)}. We have 

![](graphs/gen15.png)
  
and so 

![](graphs/gen16.png)
  
The routine is implemented in **gencont2**:

```{r}
gencont2 <- function (n, findc = F, Show = FALSE) {
  if (findc) {
    x <- seq(0, 10, length = 100)
    plot(x, 3/sqrt(pi)*sqrt(x)*exp(-x/3), type = "l")
    return(max(3/sqrt(pi) * sqrt(x) * exp(-x/3)))
  }
  X <- rep(0, n)
  for (i in 1:n) {
    for (j in 1:100) {
      Y <- rexp(1, 2/3)
      if(runif(1)<=1.34617*sqrt(Y)*exp(-Y/3)) {
          X[i] <- Y
          break
      }
   }
  }
  if (Show) {
        hist(X, breaks=100, freq=FALSE, main="")
        x <- seq(0, 10, length=250)
        lines(x, dgamma(x, 3/2, 1),
            lwd=2, col="blue")
    }
    X
}
tmp <- gencont2(10000, Show = TRUE)
```

  
why the accept-reject algorithm works for this example is illustrated on this example  in 

```{r}
accrej2.ill <- function (n) {
  f <- function(x) dgamma(x, 3/2, 1)
  g <- function(x) 2/3*exp(-2/3*x)
  c <- 3^(3/2)/sqrt(2*pi*exp(1))
  x <- seq(0, 10, length = 100)
  par(mfrow = c(1, 2))
  plot(x, f(x), 
       xlab = "x", ylab = "", type = "l", 
       ylim = c(0, 2/3), col = 2, lwd = 2)
  lines(x, g(x), col = 3, lwd = 2)
  legend(3, 2/3, c("f", "g"), lty=c(1, 1), col=c(2, 3))
  plot(x, f(x)/(c * g(x)), 
       xlab = "x", ylab = "f/(cg)", type = "l")
  for (i in 1:n) {
      Y <- rexp(1, 2/3)
      U <- runif(1)
      if (U <= f(Y)/(c * g(Y))) {
          points(Y, U, pch = "X", col = 3)
      }
      else {
          points(Y, U, pch = "0", col = 2)
      }
  }

}
accrej2.ill(50)
```

Above we picked $\lambda = 3/2$ because this matched up the means of X and Y, a reasonable choice. But is it an optimal choice? Let's see:

Optimal here means a choice of $\lambda$ that minimizes c. Now if we repeat the above calculation with $\lambda$ instead of 3/2 we find

![](graphs/gen111.png)

This is the minimum (?) and so our choice was in fact optimal.

### Generating Random Vectors

as one might expect, generating data from random vectors is generally harder than for one dimensional random variables. To begin with though, at least for the case of finite rv's there is nothing new:

say we have X = (X~1~, .., X~n~) where X~j~ takes values 

$$
x_{j1}\quad  ...  \quad x_{j n_{j}}\quad
$$

Then all we need to do is generate data from the random variable X' with values 

$$
x_{11}\quad x_{1n_{1}}\quad x_{21}\quad ...  \quad x_{n n_{n}}\quad
$$
and their respective probabilities.

**Example** generate data from the random vector (X,Y) with density

```{r echo=FALSE}
df <- data.frame(a=c(0.1, 0.2), b=c(0.5, 0.2))
colnames(df) <- c("y=0", "y=1")
rownames(df) <- c("x=0", "x=1")
kable.nice(df)
```


Instead generate data from X' with values 1-4 and probabilities (0.1, 0.5, 0.2, 0.2). Then

if X' = 1 set X = 0, Y = 0  
if X' = 2 set X = 0, Y = 1  
if X' = 3 set X = 1, Y = 0  
if X' = 4 set X = 1, Y = 1

```{r}
n <- 1e6
xprime <- sample(1:4, size=n, 
        replace=TRUE, prob=c(0.1, 0.5, 0.2, 0.2))
round(table(xprime)/n, 4)
x <- rep(0, n)
y <- x
x[xprime == 3 | xprime == 4] <- 1
y[xprime == 2 | xprime == 4] <- 1
round(table(x,y)/n, 4)
```


####**Example** 
generate data from the random vector (X,Y,Z) with density

$f(x, y, z) = (x+y+z)/162, x, y, z \in \{1, 2, 3\}$

Note that there are 27 different combinations of values of (x, y, z), so we begin by generating data from a random variable X' with values 1 - 27 and probabilities as above. 

```{r}
gendisc5 <- function(i, j, k, n=10000) {
    xyz <- matrix(0, 27, 3)
    xyz[, 1] <- rep(1:3, 9)
    xyz[, 2] <- rep(1:3, each = 3)
    xyz[, 3] <- rep(1:3, rep(9, 3))
    p <- apply(xyz, 1, sum)/162
    xprime <- sample(1:27, size=n, 
                 replace=TRUE, prob=p)
    x <- xyz[, 1][xprime]
    y <- xyz[, 2][xprime]
    z <- xyz[, 3][xprime]
    a <- 1:n
    a1 <- a[x == i]
    a2 <- a[y == j]
    a3 <- a[z == k]
    a <- table(c(a1, a2, a3))
    a <- as.numeric(names(a[a == 3]))
    c(length(a)/n, (i + j + k)/162)
}
gendisc5(1, 1, 2)
```

For infinite discrete and for continous random vectors we still have the Accept-Reject algorithm:

####**Example** 
say we want to generate data from a continuous rv (X, Y, Z) with 

$f(x, y, z) = 4xy; 0 \le x,y,z \le 1$.

Here we can generate data from (U~1~, U~2~, U~3~) = 1 on [0,1]^3^. Now 

c = max\{f(x,y,z)/g(x,y,z)\} = max\{4xy/1\} = 4

One problem in the multivariate case is to make sure that our program generates the correct data. One idea is to check the histograms of the marginals, but of course this is not sufficient proof that there is no mistake. Here the marginals are given by

![](graphs/gen18.png)

The algorithm is in **gen_xyz(1)**.

```{r}
gen_xyz <- function (which = 1, n = 10000) {
  xyz <- matrix(0, n, 3)
  if(which==1) {
      for (i in 1:n) {
         repeat {
            u <- runif(3)
            if (runif(1) <= u[1] * u[2]) 
               break
          }
          xyz[i, ] <- u
      }
  }
  if(which==2) {
    for (i in 1:n) {
      repeat {
        u <- runif(1)
        if (runif(1) <= u) 
          break
      }
      xyz[i, 1] <- u
      repeat {
        u <- runif(1)
        if(runif(1)<=u) 
          break
      }
      xyz[i, 2] <- u
    }
    xyz[, 3] <- runif(n)
  }
  par(mfrow = c(2, 2))
  hist(xyz[, 1], breaks = 100, 
       xlab = "X", ylab = "", freq = FALSE, 
        main = "")
  abline(0, 2)
  hist(xyz[, 2], breaks = 100, 
       xlab = "Y", ylab = "", freq = FALSE, 
        main = "")
  abline(0, 2)
  hist(xyz[, 3], breaks = 100, 
       xlab = "Z", ylab = "", freq = FALSE, 
        main = "")
  abline(h = 1)
  
}
gen_xyz(1)
```

In this case of course X,Y and Z are independent because

f(x, y, z) = f~X~(x) f~Y~(y) f~Z~(z)

so we can just generate data from the marginals, see **gen_xyz(2)**.

Another idea is to generate the data from conditional distributions:

####**Example** 
Say (X, Y) is a discrete rv with joint density

f(x, y) = (1-p)^2^p^x^, x,y $\in \{0, 1, ..\}, y \le x$, and 0 < p < 1.

Note that

![](graphs/gen17.png)

so we have that 

Y = G-1 

and 

X|Y=y = G+y-1

where $G\sim G(1-p)$. The method is implemented in **gen_px**.

```{r}
gen_px <- function(p, n = 10000) {
  y <- rgeom(n, 1 - p)
  x <- rgeom(n, 1 - p) + y
  z <- table(x, y)/n
  xvals <- sort(unique(x))
  yvals <- sort(unique(y))
  z1 <- matrix(0, length(xvals), length(yvals))
  dimnames(z1) <- list(xvals, yvals)
  for(i in 1:length(xvals)) 
    for (j in 1:length(yvals)) 
      if(yvals[j]<= xvals[i]) 
        z1[i, j] <- (1-p)^2*p^xvals[i]
  print(round(z1, 3))
  print(round(z, 3))
  z
}
gen_px(p=0.2)
```


####**Example** 
say we have a 10-dimensional rv with joint pdf 

$$f(x_1, ..,x_10) = c \prod x_i, 0<x_1<x_2< ... < x_{10}<1$$

For the methods we know sofar we need c:

![](graphs/gen19.png)

so this is ugly. Doable, but ugly. Of course, if we needed this for 100 instead of 10... It turns out, though, that we can actually generate such data even without knowing c, but this discussion has to wait a bit.
